import streamlit as st
import pandas as pd
import conexao
import torch
from sentence_transformers import SentenceTransformer, util

# --- CONFIGURAÃ‡ÃƒO ---
st.set_page_config(page_title="Busca SemÃ¢ntica", layout="wide")
st.title("ðŸ” Busca por Sentido (Semantic Search)")
st.markdown("Encontre tickets pelo **significado**, mesmo que nÃ£o usem as palavras exatas.")

# Verifica GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
if device == "cuda":
    st.success(f"âœ… GPU Ativada: {torch.cuda.get_device_name(0)}")
else:
    st.warning("âš ï¸ Rodando em CPU.")

# --- 1. CARGA DE DADOS ---
@st.cache_data(ttl=3600)
def carregar_dados():
    conn = conexao.conexao()
    # Trazemos uma amostra de 2000 a 5000 linhas
    return pd.read_sql("SELECT * FROM ODS_ITSM FETCH FIRST 3000 ROWS ONLY", conn)

df = carregar_dados()
if df.empty: st.stop()

# --- 2. PREPARAÃ‡ÃƒO NA BARRA LATERAL ---
st.sidebar.header("ConfiguraÃ§Ã£o")

# SeleÃ§Ã£o de Coluna AutomÃ¡tica
cols = df.columns.tolist()
idx_desc = next((i for i, c in enumerate(cols) if any(x in c.upper() for x in ['DESC', 'TEXT', 'RESUMO'])), 0)
col_texto = st.sidebar.selectbox("Coluna para analisar:", cols, index=idx_desc)

# Limpeza BÃ¡sica (importante remover vazios)
df = df.dropna(subset=[col_texto])
df = df[df[col_texto].astype(str).str.len() > 10]
df.reset_index(drop=True, inplace=True) # Reseta index para alinhar com os vetores

# --- 3. CARREGAR MODELO (NA GPU) ---
@st.cache_resource
def carregar_modelo_semantico():
    # TROCAMOS O MODELO AQUI
    # Sai o MiniLM, entra o E5-Large (Requer ~2GB de VRAM, sua placa sobra)
    return SentenceTransformer('intfloat/multilingual-e5-large', device=device)

model = carregar_modelo_semantico()

# --- 4. GERAR VETORES (EMBEDDINGS) ---
# Isso transforma os textos dos tickets em nÃºmeros.
# Cacheamos isso porque Ã© a parte "pesada".
# MUDANÃ‡A: Adicionei o nome do modelo no argumento para o cache saber diferenciar
@st.cache_data
def gerar_embeddings_banco(_model, textos_lista, model_name="e5-large"):
    return _model.encode(textos_lista, convert_to_tensor=True, show_progress_bar=True)

# ...

# Na chamada da funÃ§Ã£o:
with st.spinner("Gerando mapa semÃ¢ntico (Recalculando para E5-Large)..."):
    lista_textos = df[col_texto].astype(str).tolist()
    # Passamos o nome para forÃ§ar o Python a entender que Ã© novo
    embeddings_banco = gerar_embeddings_banco(model, lista_textos, "e5-large")

st.divider()

# --- 5. A BUSCA INTELIGENTE ---
col_search, col_btn = st.columns([0.8, 0.2])

with col_search:
    query = st.text_input(
        "Descreva o SENTIDO que vocÃª procura:",
        placeholder="Ex: Testes de validaÃ§Ã£o de sistema antes de subir para produÃ§Ã£o"
    )

with col_btn:
    st.write("") # EspaÃ§o para alinhar
    st.write("")
    buscar = st.button("ðŸ”Ž Buscar", type="primary")

if query:
    # 1. Transforma sua busca em vetor
    query_embedding = model.encode(query, convert_to_tensor=True)

    # 2. Calcula a similaridade (MatemÃ¡tica de Cosseno)
    # Compara o vetor da sua busca contra TODOS os vetores do banco instantaneamente
    scores = util.cos_sim(query_embedding, embeddings_banco)[0]

    # 3. Organiza os resultados (Top Hits)
    # Pega os Ã­ndices dos top 20 mais parecidos
    top_results = torch.topk(scores, k=50)

    st.subheader("Resultados por Similaridade")

    resultados = []
    for score, idx in zip(top_results[0], top_results[1]):
        idx = idx.item() # Converte tensor para int
        score = score.item() # Converte tensor para float

        # Filtra apenas o que tiver o mÃ­nimo de sentido (> 0.3 de similaridade)
        if score > 0.3:
            row = df.iloc[idx]
            resultados.append({
                "Similaridade (%)": f"{score*100:.1f}%",
                "Demandante": row.get('DEMANDANTE', '-'),
                "Texto Original": row[col_texto]
            })

    if resultados:
        df_result = pd.DataFrame(resultados)
        st.dataframe(df_result, use_container_width=True)
    else:
        st.warning("Nenhum ticket com sentido parecido encontrado.")